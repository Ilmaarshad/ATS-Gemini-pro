{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O8zGl_vAXaFJ",
        "outputId": "696629ca-6b03-4e02-88e9-866b1316a709"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: google.generativeai in /usr/local/lib/python3.10/dist-packages (0.3.2)\n",
            "Requirement already satisfied: google-ai-generativelanguage==0.4.0 in /usr/local/lib/python3.10/dist-packages (from google.generativeai) (0.4.0)\n",
            "Requirement already satisfied: google-auth in /usr/local/lib/python3.10/dist-packages (from google.generativeai) (2.17.3)\n",
            "Requirement already satisfied: google-api-core in /usr/local/lib/python3.10/dist-packages (from google.generativeai) (2.11.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from google.generativeai) (4.9.0)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from google.generativeai) (3.20.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from google.generativeai) (4.66.1)\n",
            "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /usr/local/lib/python3.10/dist-packages (from google-ai-generativelanguage==0.4.0->google.generativeai) (1.23.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core->google.generativeai) (1.62.0)\n",
            "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /usr/local/lib/python3.10/dist-packages (from google-api-core->google.generativeai) (2.31.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth->google.generativeai) (5.3.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth->google.generativeai) (0.3.0)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from google-auth->google.generativeai) (1.16.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth->google.generativeai) (4.9)\n",
            "Requirement already satisfied: grpcio<2.0dev,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core->google.generativeai) (1.60.1)\n",
            "Requirement already satisfied: grpcio-status<2.0.dev0,>=1.33.2 in /usr/local/lib/python3.10/dist-packages (from google-api-core->google.generativeai) (1.48.2)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth->google.generativeai) (0.5.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google.generativeai) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google.generativeai) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google.generativeai) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core->google.generativeai) (2024.2.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install google.generativeai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai\n",
        "import os\n",
        "api_key = 'AIzaSyCS6y9cYU0Kfu-yBiVPIoPEumVToquqInA'\n",
        "genai.configure(api_key=api_key)\n",
        "\n",
        "\n",
        "#genai.configure(api_key=os.environ['AIzaSyCS6y9cYU0Kfu-yBiVPIoPEumVToquqInA'])\n"
      ],
      "metadata": {
        "id": "f3GK016YZsU6"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pypdf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5cxaWk6udtxE",
        "outputId": "c79490ff-9476-4f58-ebb6-53f396146257"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pypdf\n",
            "  Downloading pypdf-4.0.1-py3-none-any.whl (283 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/284.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━\u001b[0m \u001b[32m235.5/284.0 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.0/284.0 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdf\n",
            "Successfully installed pypdf-4.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain==0.0.351"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EChR5uT4eoxD",
        "outputId": "01ac8dee-b3c3-4f5c-d979-1649ce791314"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain==0.0.351\n",
            "  Downloading langchain-0.0.351-py3-none-any.whl (794 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/794.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.4/794.3 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m794.3/794.3 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.351) (6.0.1)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.351) (2.0.25)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.351) (3.9.3)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.351) (4.0.3)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain==0.0.351)\n",
            "  Downloading dataclasses_json-0.6.4-py3-none-any.whl (28 kB)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain==0.0.351)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Collecting langchain-community<0.1,>=0.0.2 (from langchain==0.0.351)\n",
            "  Downloading langchain_community-0.0.19-py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m23.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-core<0.2,>=0.1 (from langchain==0.0.351)\n",
            "  Downloading langchain_core-0.1.22-py3-none-any.whl (239 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m239.4/239.4 kB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langsmith<0.1.0,>=0.0.70 (from langchain==0.0.351)\n",
            "  Downloading langsmith-0.0.90-py3-none-any.whl (55 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.7/55.7 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.351) (1.23.5)\n",
            "Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.351) (2.6.1)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.351) (2.31.0)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain==0.0.351) (8.2.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.351) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.351) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.351) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.351) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.351) (1.9.4)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain==0.0.351)\n",
            "  Downloading marshmallow-3.20.2-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain==0.0.351)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain==0.0.351)\n",
            "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Requirement already satisfied: anyio<5,>=3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1->langchain==0.0.351) (3.7.1)\n",
            "Collecting langsmith<0.1.0,>=0.0.70 (from langchain==0.0.351)\n",
            "  Downloading langsmith-0.0.87-py3-none-any.whl (55 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.4/55.4 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1->langchain==0.0.351) (23.2)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain==0.0.351) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain==0.0.351) (2.16.2)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain==0.0.351) (4.9.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.0.351) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.0.351) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.0.351) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain==0.0.351) (2024.2.2)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain==0.0.351) (3.0.3)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1->langchain==0.0.351) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1->langchain==0.0.351) (1.2.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain==0.0.351)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: mypy-extensions, marshmallow, jsonpointer, typing-inspect, jsonpatch, langsmith, dataclasses-json, langchain-core, langchain-community, langchain\n",
            "Successfully installed dataclasses-json-0.6.4 jsonpatch-1.33 jsonpointer-2.4 langchain-0.0.351 langchain-community-0.0.19 langchain-core-0.1.22 langsmith-0.0.87 marshmallow-3.20.2 mypy-extensions-1.0.0 typing-inspect-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import PyPDFLoader\n"
      ],
      "metadata": {
        "id": "yhdJ3w9reK57"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loader = PyPDFLoader(\"/content/Ilma_Arshad1.pdf\")\n",
        "docs = loader.load()"
      ],
      "metadata": {
        "id": "gSSBaMIxeK2q"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "type(docs)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S4mXbZiKeK1N",
        "outputId": "d4691a14-b9bb-48d5-da72-131cb19c1d7e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(docs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3inZsBgDiRDV",
        "outputId": "0e19f5fe-401a-4e8b-f78f-95ffaf32ccb7"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Document(page_content=\"ILMA ARSHAD    \\nIlmaarshad.ghs@gmail.com  | +91-6392685338 | Delhi, India | \\nlinkedin.com/in/ilmaarsha d | github.com/ilmaarshad   \\n  EXPERIENCE   \\n   \\n   iNeuron.ai   Data Scientist Intern    \\n    June 2023 – Sept 2023                    Bangalore, India   \\n• Conducted data analysis, independently building predictive models, \\ncreating visualizations, and deploying machine learning solutions on \\nAWS    \\n• Managed the end -to-end data preparation process, which involved \\ndata cleaning, ensuring data quality, and preparing it for analysis and \\nmodel development.   \\n• Deployed web application on AWS Elastic Beanstalk   \\n   \\nPERSONAL PROJECTS   \\n   \\n      \\nRetrieva l-Augmented Generation with LangChain  and  \\n   LLaMa.cp p       \\n• Implemented a custom text splitter to break down large documents into \\nmanageable chunks for enhanced processing.    \\n• Converted chunks into embeddings and stored them in ChromaDB    \\n• Implemented LangChain for advanced language processing, enriching \\ncontextual understanding.    \\n• Leveraged Llama.cpp for dynamic query and context retrieval, \\nenhancing answer precision.    \\n• Deployed on Streamlit    \\n• Tools used: LangChain, ChromaDB, LLaMa.cpp, Pandas,  PyPDF,   \\nHuggingFaceEmbeddings, Streamlit, LLM   \\n   \\nCustomer Churn Prediction Classification Mode l    \\n• Developed a classification model to predict customer churn.    \\n• Utilized historical data analysis and preprocessing for accuracy.   \\n• Employed machine learning algorithms including SVM, Logistic \\nRegression, Decision Tree and Random Forest.   \\n• Deployed on AWS   \\n• Tools : Scikit -learn, Pandas, NumPy, Matplotlib, Accuracy Score, Logistic \\nRegression, Decision Tree and Random Fore, AWS   \\n   \\n   \\n      \\n   \\n   \\n   \\n CERTIFICATES   \\n   \\n• Python for Data Science – Simplilearn    \\n• Full stack data science - iNeuron    \\n   \\nEDUCATION   \\n   \\nKalinga University, Raipur     \\nMaster's in Computer Application [MCA]   \\nAugust  2021 - September 2023   \\nUniversity of Allahabad, Prayagraj    \\nBachelor's in Commerc e [B.com]    \\nMarch 2018  - July 2021   \\n   \\nSKILLS   \\n   \\n• Machine Learning , Deep Learning , NLP, LangChain, \\nLLaMa.cpp, LLM , GenAI   \\n• PyTorch , TensorFlow    \\n• SQL, NOSQL, VectorDB    \\n• Github, MS World, MS Excel   \\n• SciKit Learn, Pandas, Numpy, Seaborn, Matplotlib, \\nNLTK, Spacy    \\n• PowerBI, Tableau    \\n• Statistics, Hypothesis Testing     \\n• Python    \\n• Data Analysis,  EDA  \\n• AWS, Azure, Docker    \\n• Flask, API    \\n\", metadata={'source': '/content/Ilma_Arshad1.pdf', 'page': 0})]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyPDF2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q9IxditUhhfF",
        "outputId": "d3fcab19-56e2-4729-e82c-d61cf1b8689e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import PyPDF2 as pdf"
      ],
      "metadata": {
        "id": "TQ6eTv37hgvw"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reader=pdf.PdfReader('/content/Ilma_Arshad1.pdf')\n",
        "text=\"\"\n",
        "for page in range(len(reader.pages)):\n",
        "              page=reader.pages[page]\n",
        "              text+=str(page.extract_text())"
      ],
      "metadata": {
        "id": "w2Ye5cqHeKze"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qM-z2yzKg8Wj",
        "outputId": "c6c72021-c17b-43e1-f713-d8cdba75257b"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ILMA ARSHAD    \n",
            "Ilmaarshad.ghs@gmail.com  | +91-6392685338 | Delhi, India | \n",
            "linkedin.com/in/ilmaarsha d | github.com/ilmaarshad   \n",
            "  EXPERIENCE   \n",
            "   \n",
            "   iNeuron.ai   Data Scientist Intern    \n",
            "    June 2023 – Sept 2023                    Bangalore, India   \n",
            "• Conducted data analysis, independently building predictive models, \n",
            "creating visualizations, and deploying machine learning solutions on \n",
            "AWS    \n",
            "• Managed the end -to-end data preparation process, which involved \n",
            "data cleaning, ensuring data quality, and preparing it for analysis and \n",
            "model development.   \n",
            "• Deployed web application on AWS Elastic Beanstalk   \n",
            "   \n",
            "PERSONAL PROJECTS   \n",
            "   \n",
            "      \n",
            "Retrieva l-Augmented Generation with LangChain  and  \n",
            "   LLaMa.cp p       \n",
            "• Implemented a custom text splitter to break down large documents into \n",
            "manageable chunks for enhanced processing.    \n",
            "• Converted chunks into embeddings and stored them in ChromaDB    \n",
            "• Implemented LangChain for advanced language processing, enriching \n",
            "contextual understanding.    \n",
            "• Leveraged Llama.cpp for dynamic query and context retrieval, \n",
            "enhancing answer precision.    \n",
            "• Deployed on Streamlit    \n",
            "• Tools used: LangChain, ChromaDB, LLaMa.cpp, Pandas,  PyPDF,   \n",
            "HuggingFaceEmbeddings, Streamlit, LLM   \n",
            "   \n",
            "Customer Churn Prediction Classification Mode l    \n",
            "• Developed a classification model to predict customer churn.    \n",
            "• Utilized historical data analysis and preprocessing for accuracy.   \n",
            "• Employed machine learning algorithms including SVM, Logistic \n",
            "Regression, Decision Tree and Random Forest.   \n",
            "• Deployed on AWS   \n",
            "• Tools : Scikit -learn, Pandas, NumPy, Matplotlib, Accuracy Score, Logistic \n",
            "Regression, Decision Tree and Random Fore, AWS   \n",
            "   \n",
            "   \n",
            "      \n",
            "   \n",
            "   \n",
            "   \n",
            " CERTIFICATES   \n",
            "   \n",
            "• Python for Data Science – Simplilearn    \n",
            "• Full stack data science - iNeuron    \n",
            "   \n",
            "EDUCATION   \n",
            "   \n",
            "Kalinga University, Raipur     \n",
            "Master's in Computer Application [MCA]   \n",
            "August  2021 - September 2023   \n",
            "University of Allahabad, Prayagraj    \n",
            "Bachelor's in Commerc e [B.com]    \n",
            "March 2018  - July 2021   \n",
            "   \n",
            "SKILLS   \n",
            "   \n",
            "• Machine Learning , Deep Learning , NLP, LangChain, \n",
            "LLaMa.cpp, LLM , GenAI   \n",
            "• PyTorch , TensorFlow    \n",
            "• SQL, NOSQL, VectorDB    \n",
            "• Github, MS World, MS Excel   \n",
            "• SciKit Learn, Pandas, Numpy, Seaborn, Matplotlib, \n",
            "NLTK, Spacy    \n",
            "• PowerBI, Tableau    \n",
            "• Statistics, Hypothesis Testing     \n",
            "• Python    \n",
            "• Data Analysis,  EDA  \n",
            "• AWS, Azure, Docker    \n",
            "• Flask, API    \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_prompt=\"\"\"\n",
        "Hey Act Like a skilled or very experience ATS(Application Tracking System)\n",
        "with a deep understanding of tech field,software engineering,data science ,data analyst\n",
        "and big data engineer. Your task is to evaluate the resume based on the given job description.\n",
        "You must consider the job market is very competitive and you should provide\n",
        "best assistance for improving thr resumes. Assign the percentage Matching based\n",
        "on Jd and\n",
        "the missing keywords with high accuracy\n",
        "resume:{text}\n",
        "description:{jd}\n",
        "\n",
        "I want the response in one single string having the structure\n",
        "{{\"JD Match\":\"%\",\"MissingKeywords:[]\",\"Profile Summary\":\"\"}}\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "QIn25zNWiK9z"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "jd =\"\"\"Data Scientist - Skill-Based Screening & Signing Bonus\n",
        "\n",
        "\n",
        "\n",
        "Location: Remote\n",
        "\n",
        "\n",
        "About Durendal Resources: Durendal Resources is pioneering the future of critical metals exploration with a state-of-the-art digital prospecting platform. Through machine learning, advanced data analytics, and extensive geological datasets, we aim to drastically increase the success rate of discovering essential mineral deposits for the electric vehicle and renewable energy sectors. Our innovative approach not only reduces environmental impact but also speeds up discovery processes and improves investment efficiency. Join us in leading the transition to a more sustainable and resource-efficient future.\n",
        "\n",
        "\n",
        "Role Overview: We are on the lookout for a Data Scientist with a passion for innovation and a knack for problem-solving. As a Data Scientist at Durendal Resources, you will analyze complex datasets to uncover insights, patterns, and opportunities for enhancing our mineral exploration processes. Your work will directly contribute to our goal of transforming the efficiency and sustainability of mineral exploration across the globe.\n",
        "\n",
        "\n",
        "Screening Assessment: To ensure we're bringing on board the best talent, candidates will undergo a skill-based screening assessment. This assessment is designed to evaluate your technical skills, problem-solving abilities, and your fit within our innovative team. It's your chance to showcase your problem-solving capabilities, technical expertise, and innovative thinking.\n",
        "\n",
        "\n",
        "Signing Bonus: To recognize the value and commitment of joining our team, the selected candidate will receive a signing bonus. We believe in rewarding those who are ready to contribute to our mission from day one.\n",
        "\n",
        "\n",
        "Key Responsibilities:\n",
        "\n",
        "Design and implement sophisticated data analysis strategies to process and interpret geological data.\n",
        "Develop predictive models and algorithms to identify promising exploration sites.\n",
        "Collaborate with a cross-functional team of geologists, engineers, and machine learning specialists to integrate data-driven insights into practical exploration strategies.\n",
        "Continually refine our data analytics methods to maintain technological leadership in digital prospecting.\n",
        "Present findings and recommendations to technical and non-technical stakeholders to guide exploration decisions.\n",
        "\n",
        "\n",
        "Qualifications:\n",
        "\n",
        "Bachelor's or Master's degree in Data Science, Statistics, Computer Science, or a related field.\n",
        "Proven experience in data modeling, analysis, and visualization.\n",
        "Expertise in programming languages such as Python or R, and familiarity with data science toolkits.\n",
        "Strong analytical skills with the ability to tackle problems creatively and effectively.\n",
        "Experience with machine learning techniques and their application to real-world problems.\n",
        "Excellent communication skills, capable of conveying complex concepts to diverse audiences.\n",
        "Background in geology or experience with geospatial data analysis is highly desirable.\"\"\""
      ],
      "metadata": {
        "id": "aESVLakzkQiJ"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model  = genai.GenerativeModel('gemini-pro')\n",
        "response=model.generate_content(input_prompt)\n",
        "print(response.text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "bgX2yb6vmwSY",
        "outputId": "403f2631-d027-4909-ba89-9f06e917ff46"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{{\"JD Match\":\"80%\",\"MissingKeywords\":[\"Data Integration\",\"Distributed Computing\",\" Machine Learning Algorithms\"],\"Profile Summary\":\"Seasoned Data Scientist with 6+ years of experience in building and deploying Machine Learning models for business intelligence and predictive analytics. Proven ability to lead and mentor teams, communicate effectively with stakeholders, and deliver results within tight deadlines.\"}}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3LPVFYx2nExv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}